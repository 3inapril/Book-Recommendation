{"cells":[{"cell_type":"code","source":["from __future__ import division\nimport pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.sql import SQLContext\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport itertools\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\nsql = pyspark.SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Read parque files \ndata = sqlContext.read.parquet(\"/FileStore/tables/se7o9yrc1496965975603/ratings_dat.parq\")\ndisplay(data)\ndata.cache()\n\nuser_data = sqlContext.read.parquet(\"/FileStore/tables/3202ecjc1498244879905/users_dat.parq\")\ndisplay(user_data)\n\nbook_data = sqlContext.read.parquet(\"/FileStore/tables/idxrkakn1498252895102/books_dat.parq\")\ndisplay(book_data)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data.schema.names\ndf = data.select(col(\"User-ID\").alias(\"userid\"), col(\"ISBN\").alias(\"isbn\"), col(\"Book-Rating\").alias(\"bookrating\"))\ndf.take(10)\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# mllib als requires that userid and bookid both integer, so here stringIndexer() is used to index string ids\n# isbn is given a new id since there are letters in isbn\nstringIndexer = StringIndexer(inputCol=\"isbn\", outputCol=\"bookid\")\nmodel = stringIndexer.fit(df)\nindexed = model.transform(df)\nindexed.show()\n\n# to convert back to string id\n#converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n#converted = converter.transform(indexed)\n#converted.show()\n\n# convert ids into integer\ndf2 = indexed.select(indexed.userid.cast(\"integer\"),\n                     indexed.bookid.cast(\"integer\"), \n                     indexed.bookrating.cast(\"integer\"))\ndf2.printSchema()\ndf2.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["print type(data)\n\nnumRatings = df2.count()\nnumUsers = df2.map(lambda r: r[0]).distinct().count()\nnumBooks = df2.map(lambda r: r[1]).distinct().count()\n\nprint \"Got %d ratings from %d users on %d books.\" % (numRatings, numUsers, numBooks)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["splits = df2.randomSplit([0.8, 0.2], 18)\ntrain_set = splits[0]\nvali_set = splits[1]\nprint \"Got %d in train_set, %d in vali_set.\" % (train_set.count(), vali_set.count())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["type(train_set), type(df2), type(data)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Check sparsity in tain_set\n# have to change train_set from dataframe to RDD in order to query\nrating_ct = train_set.filter('bookrating != 0').count()\ntrain_users_ct = train_set.map(lambda r: r[0]).distinct().count()\ntrain_books_ct = train_set.map(lambda r: r[1]).distinct().count()\nsparsity = rating_ct/(train_users_ct * train_books_ct)\nsparsity\nprint 'Sparsity: {:4.4f}%'.format(sparsity*100)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["train_set.describe('bookrating').show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["## In this section, we will use ALS.train to train a bunch of models, and select and evaluate the best. Among the training paramters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["ranks = [5, 10, 40]\nregularizations = [0.1, 1., 10., 100.]\nregularizations.sort()\niter_array = [1, 5, 10, 15]\n\nbest_params = {}\nbest_params['n_factors'] = ranks[0]\nbest_params['reg'] = regularizations[0]\nbest_params['n_iter'] = 0\nbest_params['train_mse'] = np.inf\nbest_params['test_mse'] = np.inf\nbest_params['model'] = None\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import math \nvali_set_x = vali_set.map(lambda p: (p[0], p[1]))\ntrain_set_x = train_set.map(lambda p: (p[0], p[1]))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["import math \nvali_set_x = vali_set.map(lambda p: (p[0], p[1]))\ntrain_set_x = train_set.map(lambda p: (p[0], p[1]))\n\nfor rank in ranks:\n  print 'rank: {}'.format(rank)\n  for regularization in regularizations:\n    print 'regparam: {}'.format(regularization)\n    for iter_round in iter_array:\n      print 'iter_round: {}'.format(iter_round)\n      model = ALS.train(train_set, \n                        rank=rank, iterations=iter_round, lambda_=regularization)\n      \n      predictions = model.predictAll(vali_set_x).map(lambda r: ((r[0], r[1]), r[2]))\n      ratesAndPreds = vali_set.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)      \n      RMSE =  math.sqrt(ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n      print(\"Test RMSE = \" + str(RMSE))\n      \n      train_predictions = model.predictAll(train_set_x).map(lambda r: ((r[0], r[1]), r[2]))\n      train_ratesAndPreds = train_set.map(lambda r: ((r[0], r[1]), r[2])).join(train_predictions)      \n      train_RMSE =  math.sqrt(train_ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n      print(\"Train RMSE = \" + str(train_RMSE))\n      \n      if RMSE < best_params['test_mse']:\n        print 'New optimal hyperparameters updated. New RMSE is {}'.format(RMSE)\n        best_params['n_factors'] = rank\n        best_params['reg'] = regularization\n        best_params['n_iter'] = iter_round\n        best_params['train_mse'] = train_RMSE\n        best_params['test_mse'] = RMSE\n        best_params['model'] = model  "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["best_params\n# {'model': <pyspark.mllib.recommendation.MatrixFactorizationModel at 0x7f736b71ed50>,\n# 'n_factors': 40,\n# 'n_iter': 15,\n# 'reg': 1.0,\n# 'test_mse': 3.8086599896164373,\n# 'train_mse': 2.1490012046727935}"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# plot RMSE VS. Iteration \nrank = 40\nregularization = 1.0\niter_array = [1, 2, 5, 10, 15, 20]\nRMSE_series = []\ntrain_RMSE_series = []\n\nfor iter_round in iter_array:\n  print 'iter_round: {}'.format(iter_round)\n  model = ALS.train(train_set,\n                    rank=rank, iterations=iter_round, lambda_=regularization)\n      \n  predictions = model.predictAll(vali_set_x).map(lambda r: ((r[0], r[1]), r[2]))\n  ratesAndPreds = vali_set.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n  RMSE =  math.sqrt(ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n  RMSE_series.append(RMSE)\n  print(\"Test RMSE = \" + str(RMSE))\n      \n  train_predictions = model.predictAll(train_set_x).map(lambda r: ((r[0], r[1]), r[2]))\n  train_ratesAndPreds = train_set.map(lambda r: ((r[0], r[1]), r[2])).join(train_predictions)\n  train_RMSE =  math.sqrt(train_ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n  train_RMSE_series.append(train_RMSE)\n  print(\"Train RMSE = \" + str(train_RMSE))     "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["best_params"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nax.plot(iter_array, train_RMSE_series,\n         label='Training', linewidth=3)\nax.plot(iter_array, RMSE_series,\n         label='Test', linewidth=3)\n\nax.plot(iter_array, train_RMSE_series, 'ro')\nax.plot(iter_array, RMSE_series, 'ro')\n\n# Add a legend\nax.legend()\n# Show the plot\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["model_path = './book_reco'\n# Save model\nmodel = best_params['model']\nmodel.save(sc, model_path)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["model_path = './book_reco'\nmodel = MatrixFactorizationModel.load(sc, model_path)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Besides spliting into train nad test set, we can also musk some ratings from the datasets and use these as test data"],"metadata":{}},{"cell_type":"code","source":["dat = df2\n# valid rating counts\ntype(dat)\ndat.describe()\nrating_ttl = dat.count()\nnon_zero_ct = dat.filter(dat.bookrating != 0).count()\n\nprint('%s ratings, %s non-zero ratings' %(rating_ttl, non_zero_ct))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["non_zero_ratings = dat.filter(dat.bookrating != 0)\nzero_ratings = dat.filter(dat.bookrating == 0)\n\n# sample from non_zero_ratings, split by 20%, 80%\nsplits = non_zero_ratings.randomSplit([0.8, 0.2], 18)\nnon_zero_train = splits[0]\nnon_zero_test = splits[1]\n\nnew_train = zero_ratings.unionAll(non_zero_train)\nnew_test = non_zero_test"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["new_train.take(4)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["n_factors = 40\nn_iter = 20\nreg = 1.0\nmodel = ALS.train(non_zero_train,\n                  rank=n_factors, iterations=n_iter, lambda_=reg)\ntest_x = new_test.map(lambda p: (p[0], p[1]))\npredictions = model.predictAll(test_x).map(lambda r: ((r[0], r[1]), r[2]))\nratesAndPreds = new_test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\nRMSE =  math.sqrt(ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\nprint('masked cells has test RMSE of %s' % RMSE)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### summarize rating counts by books so we can only recommend books that has ratings over a threshold"],"metadata":{}},{"cell_type":"code","source":["# print train_set.take(2)\n# train_set.map(lambda r: r[2]).distinct().collect()\n# rating 0 to 10"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["rating_ct_by_book = train_set.filter(train_set.bookrating != 0).groupBy('bookid').count().sort('count', ascending=False)\nrating_ct_by_book.take(3)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### randomly select one user in the validation set and treat it as a unseen user. Generate recommendation to this user"],"metadata":{}},{"cell_type":"code","source":["# randomly pick a user in test set\n\nnew_user = vali_set.filter(vali_set.bookrating != 0).select('userid').rdd.takeSample(False, 1, seed=0)\nnew_user\n# userid is 1848\nnew_user_ID = new_user[0][0]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# find out his unrated books\n\nrated_books = vali_set.filter(vali_set.userid == new_user_ID).filter(vali_set.bookrating != 0)\nrated_books.show()\nrated_books_ids = rated_books.map(lambda x: x[1]).collect()\ntype(rated_books_ids)\n#input for prediction new_user_x should include only the books that are NOT rated by this new user\nunrated_x = train_set.where(~col(\"bookid\").isin(rated_books_ids)).map(lambda x: (new_user_ID, x[1])).distinct()\nunrated_x.take(5)\n#new_user_pred = model.predictAll(new_user_x).map(lambda r: ((r[0], r[1]), r[2]))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from datetime import datetime\naa = sc.parallelize[('10:40:31', '10:39:31')]\nFMT = \"%H:%M:%S\"\nduration = aa.map(lambda p: (datetime.strptime(p[0], FMT) - datetime.strptime(p[1], FMT) ))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# create new traing set to retrain the model that include this new user's rated books\n\n# include these rating records of the new user into train_set and re-train the model with best_params to get new_user_model and use new_user_model to predict the scores for books that are not rated by the new user\nnew_train_set = train_set.unionAll(rated_books)\nfrom time import time\n\nt0 = time()\nnew_user_model = ALS.train(train_set,\n                  rank=40, iterations=20, lambda_=1.0)\ntime_diff = time() - t0\n\nprint \"New model trained in %s seconds\" % time_diff"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# use the new model to predict new user's rating on the books he never rated before\nnew_user_pred = new_user_model.predictAll(unrated_x).map(lambda row: (row.product, row.rating))\nnew_user_pred.take(4), type(new_user_pred) # pyspark.rdd.PipelinedRDD"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["book_data.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["print type(book_data)\nbook_data2 = book_data.select(col(\"ISBN\").alias(\"ISBN\"), col(\"Book-Title\").alias(\"book_title\"))\nstringIndexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"bookid\")\nmodel = stringIndexer.fit(book_data2)\nindexed = model.transform(book_data2)\nindexed.show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["print(type(indexed))\nbook_data2 = indexed.select(indexed.bookid.cast(\"integer\"),\n                            indexed.book_title).rdd\nbook_data2.take(3)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["rating_ct_by_book_rdd.take(3)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["new_user_pred.take(3)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["rating_ct_by_book_rdd = rating_ct_by_book.rdd\ntype(rating_ct_by_book_rdd)\nrdd_join = rating_ct_by_book_rdd.join(new_user_pred).join(book_data2)    \n# rdd join rdd, the key will by default be the first element. If there are multiple, you have to zip the rest to isolate the first one. eg. (x0,(x1,x2,x3))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["rdd_join.take(3)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["final_dat = rdd_join.map(lambda r: (r[0], r[1][1], r[1][0][0], r[1][0][1]))\nfinal_dat.take(2)\ntop10_reco = final_dat.filter(lambda x: x[2] > 30).takeOrdered(5, key=lambda x: -x[3])\ntop10_reco"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["new_user = vali_set.filter(vali_set.bookrating != 0).select('userid').rdd.takeSample(False, 1)\nprint(new_user)\n# userid is 1848\nnew_user_ID = new_user[0][0]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["vali_set.take(3)\ntype(vali_set)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["from time import time\n# write a function that takes new_user_ID and gives recommendation\ndef give_recomendation(new_user_ID):\n\n  rated_books = vali_set.filter(vali_set.userid == new_user_ID).filter(vali_set.bookrating != 0)\n  rated_books_ids = rated_books.map(lambda x: x[1]).collect()\n  new_train_set = train_set.unionAll(rated_books)\n  \n  #input for prediction new_user_x should include only the books that are NOT rated by this new user\n  # find out his unrated books\n  unrated_x = train_set.where(~col(\"bookid\").isin(rated_books_ids)).map(lambda x: (new_user_ID, x[1])).distinct()\n\n  t0 = time()\n  new_user_model = ALS.train(train_set,\n                  rank=40, iterations=20, lambda_=1.0)\n  time_diff = time() - t0\n  print \"New model trained in %s seconds\" % time_diff\n  # use the new model to predict new user's rating on the books he never rated before\n  new_user_pred = new_user_model.predictAll(unrated_x).map(lambda row: (row.product, row.rating))\n\n  rdd_join = rating_ct_by_book_rdd.join(new_user_pred).join(book_data2)\n\n  final_dat = rdd_join.map(lambda r: (r[0], r[1][1], r[1][0][0], r[1][0][1]))\n  top10_reco = final_dat.filter(lambda x: x[2] > 30).takeOrdered(15, key=lambda x: -x[3])\n  \n  return(top10_reco, rated_books)\n\nx, rated = give_recomendation(37644)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Show rated books\nrated.select(rated.bookid, rated.userid, rated.bookrating).map(lambda x: (x[0], (x[1], x[2]))).join(book_data2).collect()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# Recommended books\nx"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["vali_set.show(2)"],"metadata":{},"outputs":[],"execution_count":49}],"metadata":{"name":"book-reco-cv","notebookId":139272083591120},"nbformat":4,"nbformat_minor":0}
